{"version":3,"sources":["webpack:///./src/views/paper/LibCity-Journal.vue?e9e5","webpack:///src/views/paper/LibCity-Journal.vue","webpack:///./src/views/paper/LibCity-Journal.vue?55a2","webpack:///./src/views/paper/LibCity-Journal.vue","webpack:///./src/assets/img/framework.png","webpack:///./src/views/paper/LibCity-Journal.vue?5e9a"],"names":["render","_vm","this","_h","$createElement","_c","_self","staticClass","_v","_m","staticStyle","staticRenderFns","attrs","component","module","exports"],"mappings":"uHAAA,IAAIA,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,WAAW,CAACF,EAAG,MAAM,CAACA,EAAG,MAAMA,EAAG,IAAI,CAACE,YAAY,SAAS,CAACN,EAAIO,GAAG,wHAAwHP,EAAIQ,GAAG,GAAGR,EAAIQ,GAAG,GAAGJ,EAAG,IAAI,CAACE,YAAY,gBAAgB,CAACN,EAAIO,GAAG,cAAcH,EAAG,YAAY,CAACK,YAAY,CAAC,OAAS,SAAS,mBAAmB,2FAA2FL,EAAG,IAAI,CAACJ,EAAIO,GAAG,+gDAA+gDH,EAAG,MAAMJ,EAAIQ,GAAG,GAAGJ,EAAG,IAAI,CAACE,YAAY,gBAAgB,CAACN,EAAIO,GAAG,UAAUH,EAAG,YAAY,CAACK,YAAY,CAAC,OAAS,SAAS,mBAAmB,2FAA2FL,EAAG,IAAI,CAACJ,EAAIO,GAAG,0FAA0FP,EAAIQ,GAAG,GAAGJ,EAAG,OAAO,MAC15EM,EAAkB,CAAC,WAAa,IAAIV,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,IAAI,CAACA,EAAG,IAAI,CAACO,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACX,EAAIO,GAAG,qBAAqBP,EAAIO,GAAG,KAAKH,EAAG,IAAI,CAACO,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACX,EAAIO,GAAG,eAAe,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,IAAI,CAACJ,EAAIO,GAAG,2EAA2EH,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,IAAI,CAACJ,EAAIO,GAAG,0BAA0B,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACK,YAAY,CAAC,MAAQ,MAAM,OAAS,WAAW,aAAa,WAAW,CAACL,EAAG,MAAM,CAACO,MAAM,CAAC,IAAM,EAAQ,QAAkC,IAAM,YAAY,OAAS,YAAY,WAAa,IAAIX,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,QAAQ,CAACF,EAAG,OAAO,CAACA,EAAG,IAAI,CAACJ,EAAIO,GAAG,2BAA2BH,EAAG,IAAI,CAACK,YAAY,CAAC,cAAc,QAAQ,CAACT,EAAIO,GAAG,iIAAiIH,EAAG,IAAI,CAACK,YAAY,CAAC,cAAc,QAAQ,CAACT,EAAIO,GAAG,mGAAmGH,EAAG,IAAI,CAACK,YAAY,CAAC,cAAc,QAAQ,CAACT,EAAIO,GAAG,gDAAgDH,EAAG,IAAI,CAACK,YAAY,CAAC,cAAc,QAAQ,CAACT,EAAIO,GAAG,iBAAiBH,EAAG,IAAI,CAACJ,EAAIO,GAAG,aCsCj+C,GACE,KADF,WAEI,MAAJ,CACM,KAAN,KAGE,WAAF,IC7CwW,I,wBCQpWK,EAAY,eACd,EACAb,EACAW,GACA,EACA,KACA,WACA,MAIa,aAAAE,E,yDCnBfC,EAAOC,QAAU,IAA0B,8B,kCCA3C","file":"js/chunk-238129d3.97f55b1f.js","sourcesContent":["var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"content\"},[_c('div',[_c('br'),_c('p',{staticClass:\"title\"},[_vm._v(\"Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark\")]),_vm._m(0),_vm._m(1),_c('p',{staticClass:\"second-title\"},[_vm._v(\"Abstract\")]),_c('a-divider',{staticStyle:{\"margin\":\"10px 0\",\"background-image\":\"linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6)\"}}),_c('p',[_vm._v(\"As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temporal datasets, allowing researchers to conduct comprehensive experiments conveniently. Using LibCity, we conducted a series of experiments to validate the effectiveness of different models and components, and we summarized promising future technology developments and research directions for spatial-temporal prediction. By enabling fair model comparisons, designing a unified data storage format, and simplifying the process of developing new models, LibCity is poised to make significant contributions to the spatial-temporal prediction field.\")]),_c('br'),_vm._m(2),_c('p',{staticClass:\"second-title\"},[_vm._v(\"Cite\")]),_c('a-divider',{staticStyle:{\"margin\":\"10px 0\",\"background-image\":\"linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6)\"}}),_c('p',[_vm._v(\"If you find our work useful for your research or development, please cite our paper.\")]),_vm._m(3),_c('br')],1)])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('p',[_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2304.14343\",\"target\":\"_blank\"}},[_vm._v(\"[Official Link]\")]),_vm._v(\" \"),_c('a',{attrs:{\"href\":\"https://github.com/LibCity\",\"target\":\"_blank\"}},[_vm._v(\"[Code]\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('p',[_vm._v(\"Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chengkai Han, Wayne Xin Zhao\"),_c('br'),_c('b',[_c('i',[_vm._v(\"arXiv preprint.\")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticStyle:{\"width\":\"80%\",\"margin\":\"5px auto\",\"text-align\":\"center\"}},[_c('img',{attrs:{\"src\":require(\"../../assets/img/framework.png\"),\"alt\":\"framework\",\"height\":\"400\"}})])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"code\"},[_c('code',[_c('p',[_vm._v(\"@article{libcitylong,\")]),_c('p',{staticStyle:{\"text-indent\":\"2em\"}},[_vm._v(\"title={Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark},\")]),_c('p',{staticStyle:{\"text-indent\":\"2em\"}},[_vm._v(\"author={Jingyuan Wang and Jiawei Jiang and Wenjun Jiang and Chengkai Han and Wayne Xin Zhao},\")]),_c('p',{staticStyle:{\"text-indent\":\"2em\"}},[_vm._v(\"journal={arXiv preprint arXiv:2304.14343},\")]),_c('p',{staticStyle:{\"text-indent\":\"2em\"}},[_vm._v(\"year={2023}\")]),_c('p',[_vm._v(\"}\")])])])}]\n\nexport { render, staticRenderFns }","<template>\r\n    <div class=\"content\">\r\n        <div>\r\n            <br>\r\n            <p class=\"title\">Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark</p>\r\n            <!-- <p><a :href=\"`${path}LibCity_Journal.pdf`\" download=\"LibCity_Journal.pdf\">[Download]</a> <a href=\"https://arxiv.org/abs/2304.14343\" target=\"_blank\">[Official Link]</a> <a href=\"https://github.com/LibCity\" target=\"_blank\">[Code]</a></p> -->\r\n            <p><a href=\"https://arxiv.org/abs/2304.14343\" target=\"_blank\">[Official Link]</a> <a href=\"https://github.com/LibCity\" target=\"_blank\">[Code]</a></p>\r\n            <p>Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chengkai Han, Wayne Xin Zhao<br>\r\n            <b><i>arXiv preprint.</i></b></p>\r\n            <p class=\"second-title\">Abstract</p>\r\n            <a-divider style=\"margin: 10px 0; background-image: linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6);\"></a-divider>\r\n            <p>As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temporal datasets, allowing researchers to conduct comprehensive experiments conveniently. Using LibCity, we conducted a series of experiments to validate the effectiveness of different models and components, and we summarized promising future technology developments and research directions for spatial-temporal prediction. By enabling fair model comparisons, designing a unified data storage format, and simplifying the process of developing new models, LibCity is poised to make significant contributions to the spatial-temporal prediction field.</p>\r\n            <br>\r\n            <div style=\"width: 80%; margin: 5px auto; text-align: center\">\r\n                <img\r\n                    src=\"../../assets/img/framework.png\"\r\n                    alt=\"framework\"\r\n                    height=\"400\"\r\n                />\r\n            </div>\r\n            <p class=\"second-title\">Cite</p>\r\n            <a-divider style=\"margin: 10px 0; background-image: linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6);\"></a-divider>\r\n            <p>If you find our work useful for your research or development, please cite our paper.</p>\r\n            <div class=\"code\">\r\n                <code>\r\n                  <p>@article{libcitylong,</p>\r\n                  <p style=\"text-indent:2em\">title={Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark},</p>\r\n                  <p style=\"text-indent:2em\">author={Jingyuan Wang and Jiawei Jiang and Wenjun Jiang and Chengkai Han and Wayne Xin Zhao},</p>\r\n                  <p style=\"text-indent:2em\">journal={arXiv preprint arXiv:2304.14343},</p>\r\n                  <p style=\"text-indent:2em\">year={2023}</p>\r\n                  <p>}</p>\r\n                </code>\r\n            </div>\r\n            <br>\r\n        </div>\r\n    </div>\r\n</template>\r\n\r\n<script>\r\n    export default {\r\n        data() {\r\n            return {\r\n                path:process.env.BASE_URL,\r\n            };\r\n        },\r\n        components: {\r\n        }\r\n    };\r\n</script>\r\n\r\n<style scoped>\r\n.content {\r\n  width: 80%;\r\n  height: auto;\r\n  margin: 50px auto 0 auto;\r\n  font-size: 20px;\r\n  line-height: 36px;\r\n  color: black;\r\n  font-family: 'Open Sans', 'Microsoft YaHei', Arial, Helvetica, sans-serif;\r\n  /* border: blue solid 3px; */\r\n}\r\n.code {\r\n    color: #f8f8f2;\r\n    background-color: #272822;\r\n    tab-size: 4;\r\n    overflow: auto;\r\n    width: 100%;\r\n    padding: 10px 20px;\r\n    margin: 0px 0px 16px 0px;\r\n    text-align: left;\r\n    border: 1px solid #e5e5e5;\r\n    border-radius: 10px;\r\n    line-height: 1.5;\r\n}\r\n.title {\r\n    font-size: 32px;\r\n    font-weight: 700;\r\n    font-family: 'Open Sans', 'Microsoft YaHei', Arial, Helvetica, sans-serif;\r\n    margin-bottom: 20px;\r\n    line-height: 36px;\r\n}\r\n.second-title {\r\n    font-size: 28px;\r\n    font-weight: 700;\r\n    font-family: 'Open Sans', 'Microsoft YaHei', Arial, Helvetica, sans-serif;\r\n    margin-bottom: 0;\r\n}\r\n</style>\r\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./LibCity-Journal.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./LibCity-Journal.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./LibCity-Journal.vue?vue&type=template&id=86e2eb0c&scoped=true&\"\nimport script from \"./LibCity-Journal.vue?vue&type=script&lang=js&\"\nexport * from \"./LibCity-Journal.vue?vue&type=script&lang=js&\"\nimport style0 from \"./LibCity-Journal.vue?vue&type=style&index=0&id=86e2eb0c&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"86e2eb0c\",\n  null\n  \n)\n\nexport default component.exports","module.exports = __webpack_public_path__ + \"img/framework.85b623a8.png\";","export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./LibCity-Journal.vue?vue&type=style&index=0&id=86e2eb0c&scoped=true&lang=css&\""],"sourceRoot":""}