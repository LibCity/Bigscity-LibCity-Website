(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-238129d3"],{1286:function(e,a,t){"use strict";t.r(a);var i=function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("div",{staticClass:"content"},[t("div",[t("br"),t("p",{staticClass:"title"},[e._v("Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark")]),e._m(0),e._m(1),t("p",{staticClass:"second-title"},[e._v("Abstract")]),t("a-divider",{staticStyle:{margin:"10px 0","background-image":"linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6)"}}),t("p",[e._v("As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temporal datasets, allowing researchers to conduct comprehensive experiments conveniently. Using LibCity, we conducted a series of experiments to validate the effectiveness of different models and components, and we summarized promising future technology developments and research directions for spatial-temporal prediction. By enabling fair model comparisons, designing a unified data storage format, and simplifying the process of developing new models, LibCity is poised to make significant contributions to the spatial-temporal prediction field.")]),t("br"),e._m(2),t("p",{staticClass:"second-title"},[e._v("Cite")]),t("a-divider",{staticStyle:{margin:"10px 0","background-image":"linear-gradient(to right,  rgb(103, 179, 241),  rgb(103, 179, 241), #f6f6f6, #f6f6f6)"}}),t("p",[e._v("If you find our work useful for your research or development, please cite our paper.")]),e._m(3),t("br")],1)])},n=[function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2304.14343",target:"_blank"}},[e._v("[Official Link]")]),e._v(" "),t("a",{attrs:{href:"https://github.com/LibCity",target:"_blank"}},[e._v("[Code]")])])},function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("p",[e._v("Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chengkai Han, Wayne Xin Zhao"),t("br"),t("b",[t("i",[e._v("arXiv preprint.")])])])},function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("div",{staticStyle:{width:"80%",margin:"5px auto","text-align":"center"}},[i("img",{attrs:{src:t("8e59"),alt:"framework",height:"400"}})])},function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("div",{staticClass:"code"},[t("code",[t("p",[e._v("@article{libcitylong,")]),t("p",{staticStyle:{"text-indent":"2em"}},[e._v("title={Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark},")]),t("p",{staticStyle:{"text-indent":"2em"}},[e._v("author={Jingyuan Wang and Jiawei Jiang and Wenjun Jiang and Chengkai Han and Wayne Xin Zhao},")]),t("p",{staticStyle:{"text-indent":"2em"}},[e._v("journal={arXiv preprint arXiv:2304.14343},")]),t("p",{staticStyle:{"text-indent":"2em"}},[e._v("year={2023}")]),t("p",[e._v("}")])])])}],r={data:function(){return{path:""}},components:{}},o=r,s=(t("c7a9"),t("2877")),l=Object(s["a"])(o,i,n,!1,null,"86e2eb0c",null);a["default"]=l.exports},"89aa":function(e,a,t){},"8e59":function(e,a,t){e.exports=t.p+"img/framework.85b623a8.png"},c7a9:function(e,a,t){"use strict";t("89aa")}}]);
//# sourceMappingURL=chunk-238129d3.97f55b1f.js.map